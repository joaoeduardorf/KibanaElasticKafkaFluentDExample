input {
  kafka {
    bootstrap_servers => "kafka:9092"
    topics => ["app.logs"]
    group_id => "logstash-consumer-group-1"
    codec => json
  }
}

filter {
  json {
    source => "log"
    target => "parsed_log"
    skip_on_invalid_json => true
  }

  mutate {
    add_field => {
      "timestamp" => "%{[parsed_log][Timestamp]}"
      "level" => "%{[parsed_log][Level]}"
      "message_template" => "%{[parsed_log][MessageTemplate]}"
      "trace_id" => "%{[parsed_log][TraceId]}"
      "span_id" => "%{[parsed_log][SpanId]}"
      "elapsed_milliseconds" => "%{[parsed_log][Properties][ElapsedMilliseconds]}"
      "status_code" => "%{[parsed_log][Properties][StatusCode]}"
      "content_type" => "%{[parsed_log][Properties][ContentType]}"
      "content_length" => "%{[parsed_log][Properties][ContentLength]}"
      "protocol" => "%{[parsed_log][Properties][Protocol]}"
      "method" => "%{[parsed_log][log][Properties][Method]}"
      "scheme" => "%{[parsed_log][Properties][Scheme]}"
      "host" => "%{[parsed_log][Properties][Host]}"
      "path" => "%{[parsed_log][Properties][Path]}"
      "query_string" => "%{[parsed_log][Properties][QueryString]}"
      "request_id" => "%{[parsed_log][Properties][RequestId]}"
      "connection_id" => "%{[parsed_log][Properties][ConnectionId]}"
    }
  }

  date {
    match => ["timestamp", "ISO8601"]
    target => "@timestamp"
  }

  mutate {
    remove_field => ["parsed_log", "log"]
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "my-space-application-logs-1"
  }

  stdout {
    codec => rubydebug
  }
}
